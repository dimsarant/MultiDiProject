{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apr0vleptos/MultiDiProject/blob/master/report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38Tt0HJEkTbx",
        "colab_type": "text"
      },
      "source": [
        "# LSH Algorith with Cosine Document similarity\n",
        "\n",
        "The part of the Locality-Sensitive-Hashing Algorithm consists of 3 parts,as we can see in the picture below:\n",
        "\n",
        "\n",
        "*   Turn each documment in sets of k-shinglings.\n",
        "*   Through, the MinHashing Algorithm we turn the above sets into Signatures.\n",
        "*   Finally, with LSH we hash signatures into backets.\n",
        "\n",
        "\n",
        "![lsh_diagramm](https://miro.medium.com/max/1904/1*27nQOTC79yfh5lzmL06Ieg.png)\n",
        "\n",
        "Finally, the candidate pairs (output of LSH) are the input in our Cosine Similarity Algorithm, in order to find the percentage of similarity of document pairs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUXe9DXt0701",
        "colab_type": "text"
      },
      "source": [
        "Our implementation is in Python 3. The pilars that are mentioned above, are all implemented seperatly (functional). Then we combine all of the above, in our main script. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfWkmvuS4wSN",
        "colab_type": "text"
      },
      "source": [
        "# k-Shingling\n",
        "\n",
        "The shingling.py file contains 3 functions, that will be descibed below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCagzfpA7KsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function that breaks a text into k-shinglings and returns \n",
        "# a list of the MD5 values of each shingling.\n",
        "def get_shingles(text, k):\n",
        "    list = [md5(text[i:i + k].encode('ascii')).hexdigest()\n",
        "            for i in range(len(text) - k)]\n",
        "    unique_list = reduce(\n",
        "        lambda x, y: x + [y] if x == [] or x[-1] != y else x, list, [])\n",
        "    return unique_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJu6d1Ef7iN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function that decides the K, depending the text.\n",
        "def shingles(text):\n",
        "    if len(text) > 1000:\n",
        "        return get_shingles(text, 10)\n",
        "    return get_shingles(text, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPjy489R76JM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function that creates a \"universe\" of shinglings and constructs a \n",
        "# matrix of shinglings for each document\n",
        "def getSinglingMatrix(text_list):\n",
        "    universe = set()\n",
        "    for text in tqdm(text_list, \"Creating Universe dictionary from each document\"):\n",
        "        for shingle in shingles(text):\n",
        "            universe.add(shingle)\n",
        "\n",
        "    universe = sorted(universe)\n",
        "    print(\"Universe of shingles got sorted\")\n",
        "\n",
        "    shingle_array = [[1 if shingle in shingles(doc_text) else 0 for doc_text in text_list] for shingle in tqdm(\n",
        "        universe, \"Creating Shingling matrix\")]\n",
        "\n",
        "    print(\"Shingling Matrix just got created for all documents\")\n",
        "    return shingle_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfPYrEGD8de5",
        "colab_type": "text"
      },
      "source": [
        "# MinHash\n",
        "\n",
        "The minhash.py file contains 2 functions, that will be descibed below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9SX1Yjq9OAL",
        "colab_type": "text"
      },
      "source": [
        "Function that creates a number of hash functions. The produced hash functions are type of:\n",
        ">$f(x) = (a x + b)\\mod k$  \n",
        "\n",
        "\n",
        "where $a,b$ are random integers (from 0 to 32)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIKq9AGD84hw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getHashfunction(num_of_hash_fun, len_of_array):\n",
        "    return [lambda x: (randint(0, 32) * x + randint(0, 32)) % len_of_array for _ in tqdm(range(num_of_hash_fun), \"Creating {} hash functions\".format(num_of_hash_fun))]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCdSYOJo-5KN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function that turns the Shingling matrix into signatures, with the help \n",
        "# of the getHashfunction\n",
        "def getSignatures(boolean_mat, num_of_hash_fun):\n",
        "    hash_func_table = getHashfunction(num_of_hash_fun, len(boolean_mat))\n",
        "\n",
        "    signatures = [[len(boolean_mat)] * (num_of_hash_fun)\n",
        "                  for _ in range(len(boolean_mat[0]))]\n",
        "\n",
        "    for index, row in enumerate(boolean_mat):\n",
        "        for hash_index, hash_func in enumerate(hash_func_table):\n",
        "            for col_index, column in enumerate(row):\n",
        "                if column == 1:\n",
        "                    if hash_func(index) < signatures[col_index][hash_index]:\n",
        "                        signatures[col_index][hash_index] = hash_func(index)\n",
        "    return signatures"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZId2VOqI_cFS",
        "colab_type": "text"
      },
      "source": [
        "#Locality Sensitive Hashing\n",
        "\n",
        "The lsh.py file contains 2 functions, that will be descibed below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPvhHG-lCTiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function that hashes sets of signatures into buckets\n",
        "def filter_buckets(signature_matrix):\n",
        "    bands_num = 20\n",
        "    num_of_buckets = 2 * ceil(sqrt(len(signature_matrix)))\n",
        "    if num_of_buckets <= 100:\n",
        "        num_of_buckets = 100\n",
        "    total_bucket_list = []\n",
        "    for band in tqdm(range(0, len(signature_matrix[0]), 5), \"Creating buckets for each band\"):\n",
        "        band_list = [[] for _ in range(num_of_buckets)]\n",
        "        for doc_idx in range(len(signature_matrix)):\n",
        "            band_list[abs(hash(tuple(signature_matrix[doc_idx][band:band + 5]))) % num_of_buckets].append(doc_idx)\n",
        "        total_bucket_list.append(band_list)\n",
        "    print(\"Bucket List just got created for each band\")\n",
        "    return total_bucket_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JTtoVHIS9xf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function that creates an upper triangular matrix that stores the number\n",
        "# of times that 2 documents were on the same bucket.\n",
        "def calculate_similarity(total_bucket_list, num_of_docs):\n",
        "    occurance_matrix = [[0] * num_of_docs for _ in range(num_of_docs)]\n",
        "    for band in total_bucket_list:\n",
        "        for index, bucket in enumerate(band):\n",
        "            if bucket:\n",
        "                for comb in combinations(bucket, 2):\n",
        "                    occurance_matrix[comb[0]][comb[1]] += 1\n",
        "                if index == 0:\n",
        "                    for comb in combinations(sorted(band[index] + band[index + 1]), 2):\n",
        "                        occurance_matrix[comb[0]][comb[1]] += 1\n",
        "                elif index == len(band) - 1:\n",
        "                    for comb in combinations(sorted(band[index - 1] + band[index]), 2):\n",
        "                        occurance_matrix[comb[0]][comb[1]] += 1\n",
        "                else:\n",
        "                    for comb in combinations(sorted(band[index - 1] + band[index] + band[index + 1]), 2):\n",
        "                        occurance_matrix[comb[0]][comb[1]] += 1\n",
        "    return occurance_matrix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF0s9z0PBgEp",
        "colab_type": "text"
      },
      "source": [
        "# Cosine Similarity\n",
        "\n",
        "The cosim.py file contains a function that implements the algorithm. Our implementation can find calculate the similarities for many files, not just for 2. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZljfwDlCfNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosign_sim(text_list):\n",
        "    unique_words = []\n",
        "    doc_words = []\n",
        "    for text in text_list:\n",
        "        words = re.split(r'\\W+', text)\n",
        "        doc_words.append(words)\n",
        "        for word in words:\n",
        "            if word not in unique_words:\n",
        "                unique_words.append(word)\n",
        "    unique_words.sort()\n",
        "\n",
        "    # bazw tis times emfanishs ths kathe lekshs gia kathe arxeio\n",
        "    doc_appear = []\n",
        "    for i in range(len(text_list)):\n",
        "        doc_appear.append([0] * len(unique_words))\n",
        "        for j in range(len(doc_words[i])):\n",
        "            for k in range(len(unique_words)):\n",
        "                if(unique_words[k] == doc_words[i][j]):\n",
        "                    doc_appear[i][k] += 1\n",
        "    # briskw thn omoiothta metaksi twn arxeiwn\n",
        "    doc_abs = []\n",
        "    for i in range(len(doc_appear)):\n",
        "        temp = 0\n",
        "        for j in range(len(doc_appear[i])):\n",
        "            temp += doc_appear[i][j]**2\n",
        "        temp = temp**(0.5)\n",
        "        doc_abs.append(temp)\n",
        "    similarity = []\n",
        "    # upologismos similarities\n",
        "    for i in range(len(text_list)):\n",
        "        for j in range(i + 1, len(doc_appear)):\n",
        "            mul = 0\n",
        "            for k in range(len(doc_appear[i])):\n",
        "                mul += doc_appear[i][k] * doc_appear[j][k]\n",
        "            similarity.append(round(100 * mul / (doc_abs[i] * doc_abs[j]), 2))\n",
        "    return similarity[0]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}